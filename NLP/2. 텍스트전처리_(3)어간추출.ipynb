{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4079fa",
   "metadata": {},
   "source": [
    "## 어간 추출(Stemming) and 표제어 추출(Lemmatization)\n",
    "- 정규화 기법 중 코퍼스에 있는 단어의 개수를 줄일 수 있는 기법\n",
    "- 눈으로 봤을 때는 서로 다른 단어들이지만, 하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이겠다는 것\n",
    "- 단어의 빈도수를 기반으로 문제를 풀고자 하는 뒤에서 학습하게 될 BoW(Bag of Words) 표현을 사용하는 자연어 처리 문제에서 주로 사용\n",
    "- 정규화의 지향점은 언제나 갖고 있는 코퍼스로부터 복잡성을 줄이는 일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8d2ae",
   "metadata": {},
   "source": [
    "### 표제어 추출(Lemmatization)\n",
    "- 표제어(Lemma)는 한글로는 '표제어' 또는 '기본 사전형 단어' 정도의 의미\n",
    "- 표제어 추출은 단어들로부터 표제어를 찾아가는 과정\n",
    "    - ex. am, are, is의 뿌리 단어(=표제어): be\n",
    "- 표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것\n",
    "    - 형태소: '의미를 가진 가장 작은 단위'\n",
    "    - 형태학(morphology): 형태소로부터 단어들을 만들어가는 학문을 뜻\n",
    "    - 형태소의 종류: 어간(stem)과 접사(affix)\n",
    "- 어간(stem): 단어의 의미를 담고 있는 단어의 핵심 부분\n",
    "- 접사(affix): 단어에 추가적인 의미를 주는 부분\n",
    "    - cats를 형태학적 파싱 -> cat(어간), -s(접사)분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca142cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에러 난다면\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25794cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "# NLTK에서는 표제어 추출을 위한 도구인 WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print('표제어 추출 전 :',words)\n",
    "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66604597",
   "metadata": {},
   "source": [
    "- 표제어 추출은 뒤에서 언급할 어간 추출과는 달리 단어의 형태가 적절히 보존되는 양상을 보이는 특징\n",
    "- 하지만, dy나 ha와 같이 의미를 알 수 없는 적절하지 못한 단어를 출력\n",
    "    -  표제어 추출기(lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문\n",
    "- WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있음\n",
    "- 표제어 추출은 문맥을 고려하며 수행했을 때의 결과는 해당 단어의 품사 정보를 보존\n",
    "- 하지만, 어간 추출의 결과는 품사 정보가 보존되지 않음\n",
    "- 어간 추출을 한 결과는 사전에 존재하지 않는 단어일 경우가 많음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47064a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50ef263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a485b70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af9366",
   "metadata": {},
   "source": [
    "### 어간 추출(Stemming)\n",
    "- 어간 추출(stemming): 어간(Stem)을 추출하는 작업\n",
    "    - 어간 추출은 형태학적 분석을 단순화한 버전\n",
    "    - 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업\n",
    "- 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b4462d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "# 어간 추출 알고리즘 포터 알고리즘(Porter Algorithm\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "print('어간 추출 전 :', tokenized_sentence)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46bc3a9",
   "metadata": {},
   "source": [
    "- 규칙 기반의 접근 -> 사전에 없는 단어들도 포함\n",
    "- 포터 알고리즘의 어간 추출 규칙\n",
    "    - ALIZE → AL\n",
    "    - ANCE → 제거\n",
    "    - ICAL → IC\n",
    "- 규칙에 따라 아래 예시결과\n",
    "    - formalize → formal\n",
    "    - allowance → allow\n",
    "    - electricical → electric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45738657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARTIN_EXTENSIONS',\n",
       " 'NLTK_EXTENSIONS',\n",
       " 'ORIGINAL_ALGORITHM',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_apply_rule_list',\n",
       " '_contains_vowel',\n",
       " '_ends_cvc',\n",
       " '_ends_double_consonant',\n",
       " '_has_positive_measure',\n",
       " '_is_consonant',\n",
       " '_measure',\n",
       " '_replace_suffix',\n",
       " '_step1a',\n",
       " '_step1b',\n",
       " '_step1c',\n",
       " '_step2',\n",
       " '_step3',\n",
       " '_step4',\n",
       " '_step5a',\n",
       " '_step5b',\n",
       " 'mode',\n",
       " 'pool',\n",
       " 'stem',\n",
       " 'vowels']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(stemmer) #궁금해서 출력해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06335326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['formalize', 'allowance', 'electricical']\n",
      "어간 추출 후 : ['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words = ['formalize', 'allowance', 'electricical']\n",
    "\n",
    "print('어간 추출 전 :',words)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6ff06",
   "metadata": {},
   "source": [
    "- 어간 추출 속도는 표제어 추출보다 일반적으로 빠름\n",
    "- 포터 어간 추출기는 정밀하게 설계되어 정확도가 높음\n",
    "    - 영어 자연어 처리에서 어간 추출하고자 한다면 준수한 선택지\n",
    "- NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer) 알고리즘을 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55abce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print('어간 추출 전 :', words)\n",
    "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
    "print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54231c",
   "metadata": {},
   "source": [
    "- 두 스태머는 동일한 단어들의 나열에 대해서 전혀 다른 결과를 보여줌\n",
    "    - 서로 다른 알고리즘을 사용하기 때문에\n",
    "- 이미 알려진 알고리즘을 사용할 때는, 사용하고자 하는 코퍼스에 스태머를 적용해보고 어떤 스태머가 해당 코퍼스에 적합한지를 판단한 후에 사용\n",
    "- 이런 규칙에 기반한 알고리즘은 종종 제대로 된 일반화를 수행하지 못 할 수 있음\n",
    "    - 어간 추출을 하고나서 일반화가 지나치게 되거나, 또는 덜 되거나 하는 경우\n",
    "    - ex. 포터 알고리즘에서 organization → organ 의 결과를 얻음. 완전히 다른 단어를 출력\n",
    "    - 의미가 동일한 경우에만 같은 단어를 얻기를 원하는 정규화의 목적에는 맞지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865bc1b3",
   "metadata": {},
   "source": [
    "- 마지막으로, 동일한 단어에 대해서 표제어 추출과 어간 추출을 수행했을 때의 예시 결과\n",
    "- Stemming\n",
    "    - am → am\n",
    "    - the going → the go\n",
    "    - having → hav\n",
    "- Lemmatization\n",
    "    - am → be\n",
    "    - the going → the going\n",
    "    - having → have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191dcf74",
   "metadata": {},
   "source": [
    "### 한국어에서의 어간 추출\n",
    "- 한국어는 5언 9품사의 구조\n",
    "    - 체언(언): 명사, 대명사, 수사(품사)\n",
    "    - 수식언(언): 관형사, 부사(품사)\n",
    "    - 관계언(언): 조사(품사)\n",
    "    - 독립언(언): 감탄사(품사)\n",
    "    - 용언(언): 동사, 형용사(품사)\n",
    "- 용언에 해당되는 '동사'와 '형용사'는 어간(stem)과 어미(ending)의 결합으로 구성\n",
    "\n",
    "#### 활용(conjugation)\n",
    "- 활용(conjugation)은 한국어에서만 가지는 특징이 아니라, 인도유럽어(indo-european language)에서도 주로 볼 수 있는 언어적 특징 중 하나\n",
    "- 활용: 용언의 어간(stem)이 어미(ending)를 가지는 일\n",
    "    - 어간(stem): 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(예: 긋다, 긋고, 그어서, 그어라)\n",
    "    - 어미(ending): 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행\n",
    "- 활용은 어간이 어미를 취할 때, 어간의 모습이 일정하다면 규칙 활용, 어간이나 어미의 모습이 변하는 불규칙 활용으로 나뉨\n",
    "\n",
    "#### 규칙 활용\n",
    "- 어간이 어미를 취할 때, 어간의 모습이 일정\n",
    "    - ex. 잡/어간 + 다/어미\n",
    "- 어간 추출: 규칙 기반으로 어미를 단순히 분리 \n",
    "\n",
    "#### 불규칙 활용\n",
    "- 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우\n",
    "    - ex. ‘듣/들-, 돕/도우-, 곱/고우-, 잇/이-, 올/올-, 노랗/노라-’\n",
    "    - ex. '오르+ 아/어→올라, 하+아/어→하여, 이르+아/어→이르러, 푸르+아/어→푸르러'\n",
    "    - 일반적인 어미가 아닌 특수한 어미를 취하는 경우\n",
    "- 어간 추출: 단순한 분리만이 아닌 좀 더 복잡한 규칙 필요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
