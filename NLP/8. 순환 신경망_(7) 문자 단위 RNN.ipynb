{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d1f4aabd",
   "metadata": {},
   "source": [
    "## 문자 단위 RNN(Char RNN)\n",
    "- https://wikidocs.net/48649\n",
    "- RNN은 전부 입력과 출력의 단위가 단어 벡터\n",
    "- 입출력의 단위를 단어 레벨(word-level)에서 문자 레벨(character-level)로 변경하여 RNN을 구현 가능\n",
    "- 문자 단위 RNN을 다 대 다(Many-to-Many) 구조로 구현한 경우, 다 대 일(Many-to-One) 구조로 구현한 경우\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9d706",
   "metadata": {},
   "source": [
    "### 문자 단위 RNN 언어 모델(Char RNNLM)\n",
    "- 단어 단위 RNN 언어 모델과 다른 점은 단어 단위가 아니라 문자 단위를 입, 출력으로 사용\n",
    "- 임베딩층(embedding layer)을 여기서는 사용 X\n",
    "- 언어 모델의 훈련 과정과 테스트 과정의 차이를 이해하는데 집중하기~!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9710cb",
   "metadata": {},
   "source": [
    "#### 데이터에 대한 이해와 전처리\n",
    "- 특수문자 제거, 단어 소문자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed9ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 데이터 로드\n",
    "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")\n",
    "\n",
    "f = open('11-0.txt', 'rb')\n",
    "sentences = []\n",
    "for sentence in f: # 데이터로부터 한 줄씩 읽는다.\n",
    "    sentence = sentence.strip() # strip()을 통해 \\r, \\n을 제거한다.\n",
    "    sentence = sentence.lower() # 소문자화.\n",
    "    sentence = sentence.decode('ascii', 'ignore') # \\xe2\\x80\\x99 등과 같은 바이트 열 제거\n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58bf2c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the project gutenberg ebook of alices adventures in wonderland, by lewis carroll',\n",
       " 'this ebook is for the use of anyone anywhere in the united states and',\n",
       " 'most other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever. you may copy it, give it away or re-use it under the terms',\n",
       " 'of the project gutenberg license included with this ebook or online at']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a612e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자열의 길이 또는 총 문자의 개수: 159484\n"
     ]
    }
   ],
   "source": [
    "# 하나의 문자열로 통합\n",
    "# 문자열의 길이는 약 15만 9천\n",
    "total_data = ' '.join(sentences)\n",
    "print('문자열의 길이 또는 총 문자의 개수: %d' % len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc4e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the project gutenberg ebook of alices adventures in wonderland, by lewis carroll this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with\n"
     ]
    }
   ],
   "source": [
    "print(total_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76a393b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 56\n"
     ]
    }
   ],
   "source": [
    "# 문자열로부터 문자 집합 생성\n",
    "# 기존에는 중복 제거 단어 집함 생성\n",
    "# 이번엔 문자집함 생성\n",
    "char_vocab = sorted(list(set(total_data)))\n",
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ef3ea",
   "metadata": {},
   "source": [
    "- 문자 집합의 크기는 단어 집합을 사용했을 경우보다 집합의 크기가 현저히 작습니다\n",
    "    - 알파벳 26개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad94de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합 : {' ': 0, '!': 1, '\"': 2, '#': 3, '$': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '[': 27, ']': 28, '_': 29, 'a': 30, 'b': 31, 'c': 32, 'd': 33, 'e': 34, 'f': 35, 'g': 36, 'h': 37, 'i': 38, 'j': 39, 'k': 40, 'l': 41, 'm': 42, 'n': 43, 'o': 44, 'p': 45, 'q': 46, 'r': 47, 's': 48, 't': 49, 'u': 50, 'v': 51, 'w': 52, 'x': 53, 'y': 54, 'z': 55}\n"
     ]
    }
   ],
   "source": [
    "# 문자 집합의 각 문자에 정수를 부여\n",
    "char_to_index = dict((char, index) for index, char in enumerate(char_vocab))\n",
    "print('문자 집합 :',char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aad7ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수로부터 문자를 리턴하는 index_to_char 생성\n",
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26588b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: '!', 2: '\"', 3: '#', 4: '$', 5: '%', 6: \"'\", 7: '(', 8: ')', 9: '*', 10: ',', 11: '-', 12: '.', 13: '/', 14: '0', 15: '1', 16: '2', 17: '3', 18: '4', 19: '5', 20: '6', 21: '7', 22: '8', 23: '9', 24: ':', 25: ';', 26: '?', 27: '[', 28: ']', 29: '_', 30: 'a', 31: 'b', 32: 'c', 33: 'd', 34: 'e', 35: 'f', 36: 'g', 37: 'h', 38: 'i', 39: 'j', 40: 'k', 41: 'l', 42: 'm', 43: 'n', 44: 'o', 45: 'p', 46: 'q', 47: 'r', 48: 's', 49: 't', 50: 'u', 51: 'v', 52: 'w', 53: 'x', 54: 'y', 55: 'z'}\n"
     ]
    }
   ],
   "source": [
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fd1ee",
   "metadata": {},
   "source": [
    "- 훈련 데이터 구성\n",
    "    - apple라는 시퀀스, 입력 길이 4라고 가정\n",
    "        - 입력 길이 4 -> 입력 시퀀스, 예츠갷야하는 출력 시퀀스 모두 4\n",
    "        - RNN 총 4번의 시점을 가질 수 있음\n",
    "        - apple 은 다섯글자이지만 입력 길이 4이므로 'appl'까지만 입력으로 사용 가능\n",
    "        - 언어 모델은 다음 시점의 입력을 예측해야하는 모델 -> 'pple'를 예측하도록 데이터가 구성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d2702bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appl (입력 시퀀스) -> pple (예측해야하는 시퀀스)\n",
    "train_X = 'appl'\n",
    "train_y = 'pple'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a014f2e",
   "metadata": {},
   "source": [
    "- 다수 샘플 생성\n",
    "- 샘플 길이 정하고 -> 문자열 전쳉에 해당 길이만큼 등분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62459558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 수 : 2658\n"
     ]
    }
   ],
   "source": [
    "# 문장의 길이 60\n",
    "seq_length = 60\n",
    "\n",
    "# 문자열의 길이를 seq_length로 나누면 전처리 후 생겨날 샘플 수\n",
    "n_samples = int(np.floor((len(total_data) - 1) / seq_length))\n",
    "print ('샘플의 수 : {}'.format(n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9c4de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 진행\n",
    "train_X = []\n",
    "train_y = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # 0:60 -> 60:120 -> 120:180로 loop를 돌면서 문장 샘플을 1개씩 pick.\n",
    "    X_sample = total_data[i * seq_length: (i + 1) * seq_length]\n",
    "\n",
    "    # 정수 인코딩\n",
    "    X_encoded = [char_to_index[c] for c in X_sample]\n",
    "    train_X.append(X_encoded)\n",
    "\n",
    "    # 오른쪽으로 1칸 쉬프트\n",
    "    y_sample = total_data[i * seq_length + 1: (i + 1) * seq_length + 1]\n",
    "    y_encoded = [char_to_index[c] for c in y_sample]\n",
    "    train_y.append(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dac1f4",
   "metadata": {},
   "source": [
    "- train_y[0]은 train_X[0]에서 오른쪽으로 한 칸 쉬프트 된 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "019834fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터의 첫번째 샘플 : [49, 37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30]\n",
      "y 데이터의 첫번째 샘플 : [37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30, 43]\n",
      "--------------------------------------------------\n",
      "X 데이터의 첫번째 샘플 디코딩 : ['t', 'h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a']\n",
      "y 데이터의 첫번째 샘플 디코딩 : ['h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a', 'n']\n"
     ]
    }
   ],
   "source": [
    "print('X 데이터의 첫번째 샘플 :',train_X[0])\n",
    "print('y 데이터의 첫번째 샘플 :',train_y[0])\n",
    "print('-'*50)\n",
    "print('X 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_X[0]])\n",
    "print('y 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95862fe",
   "metadata": {},
   "source": [
    "- train_X와 train_y에 대해서 원-핫 인코딩을 수행\n",
    "- 문자 단위 RNN에서는 입력 시퀀스에 대해서 워드 임베딩X \n",
    "    - 입력 시퀀스에도 원-핫 인코딩 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a7508da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X의 크기(shape) : (2658, 60, 56)\n",
      "train_y의 크기(shape) : (2658, 60, 56)\n"
     ]
    }
   ],
   "source": [
    "train_X = to_categorical(train_X)\n",
    "train_y = to_categorical(train_y)\n",
    "\n",
    "print('train_X의 크기(shape) : {}'.format(train_X.shape)) # 원-핫 인코딩\n",
    "print('train_y의 크기(shape) : {}'.format(train_y.shape)) # 원-핫 인코딩"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "fd909a86",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aacde19",
   "metadata": {},
   "source": [
    "- 샘플 수(No. of samples) : 2658개\n",
    "- 입력 시퀀스 길이(input_length): 60\n",
    "- 벡터의 차원(input_dim): 55? 56인듯?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94123f70",
   "metadata": {},
   "source": [
    "#### 모델 설계하기\n",
    "- 하이퍼파라미터인 은닉 상태의 크기는 256\n",
    "- 다 대 다 구조의 LSTM을 사용\n",
    "- LSTMP 은닉층 2개 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5926f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "84/84 - 23s - loss: 3.0708 - accuracy: 0.1812 - 23s/epoch - 270ms/step\n",
      "Epoch 2/80\n",
      "84/84 - 21s - loss: 2.7093 - accuracy: 0.2538 - 21s/epoch - 246ms/step\n",
      "Epoch 3/80\n",
      "84/84 - 20s - loss: 2.3588 - accuracy: 0.3379 - 20s/epoch - 243ms/step\n",
      "Epoch 4/80\n",
      "84/84 - 20s - loss: 2.2167 - accuracy: 0.3718 - 20s/epoch - 240ms/step\n",
      "Epoch 5/80\n",
      "84/84 - 21s - loss: 2.1238 - accuracy: 0.3922 - 21s/epoch - 248ms/step\n",
      "Epoch 6/80\n",
      "84/84 - 21s - loss: 2.0338 - accuracy: 0.4165 - 21s/epoch - 249ms/step\n",
      "Epoch 7/80\n",
      "84/84 - 20s - loss: 1.9639 - accuracy: 0.4337 - 20s/epoch - 244ms/step\n",
      "Epoch 8/80\n",
      "84/84 - 21s - loss: 1.8990 - accuracy: 0.4528 - 21s/epoch - 246ms/step\n",
      "Epoch 9/80\n",
      "84/84 - 20s - loss: 1.8417 - accuracy: 0.4679 - 20s/epoch - 243ms/step\n",
      "Epoch 10/80\n",
      "84/84 - 20s - loss: 1.7879 - accuracy: 0.4832 - 20s/epoch - 240ms/step\n",
      "Epoch 11/80\n",
      "84/84 - 21s - loss: 1.7421 - accuracy: 0.4940 - 21s/epoch - 244ms/step\n",
      "Epoch 12/80\n",
      "84/84 - 21s - loss: 1.6998 - accuracy: 0.5050 - 21s/epoch - 247ms/step\n",
      "Epoch 13/80\n",
      "84/84 - 20s - loss: 1.6558 - accuracy: 0.5160 - 20s/epoch - 243ms/step\n",
      "Epoch 14/80\n",
      "84/84 - 20s - loss: 1.6176 - accuracy: 0.5255 - 20s/epoch - 243ms/step\n",
      "Epoch 15/80\n",
      "84/84 - 20s - loss: 1.5815 - accuracy: 0.5345 - 20s/epoch - 241ms/step\n",
      "Epoch 16/80\n",
      "84/84 - 20s - loss: 1.5462 - accuracy: 0.5439 - 20s/epoch - 240ms/step\n",
      "Epoch 17/80\n",
      "84/84 - 21s - loss: 1.5109 - accuracy: 0.5538 - 21s/epoch - 244ms/step\n",
      "Epoch 18/80\n",
      "84/84 - 20s - loss: 1.4776 - accuracy: 0.5623 - 20s/epoch - 243ms/step\n",
      "Epoch 19/80\n",
      "84/84 - 21s - loss: 1.4477 - accuracy: 0.5706 - 21s/epoch - 246ms/step\n",
      "Epoch 20/80\n",
      "84/84 - 20s - loss: 1.4150 - accuracy: 0.5806 - 20s/epoch - 244ms/step\n",
      "Epoch 21/80\n",
      "84/84 - 20s - loss: 1.3851 - accuracy: 0.5894 - 20s/epoch - 241ms/step\n",
      "Epoch 22/80\n",
      "84/84 - 20s - loss: 1.3565 - accuracy: 0.5973 - 20s/epoch - 241ms/step\n",
      "Epoch 23/80\n",
      "84/84 - 20s - loss: 1.3230 - accuracy: 0.6075 - 20s/epoch - 243ms/step\n",
      "Epoch 24/80\n",
      "84/84 - 21s - loss: 1.2941 - accuracy: 0.6153 - 21s/epoch - 246ms/step\n",
      "Epoch 25/80\n",
      "84/84 - 21s - loss: 1.2657 - accuracy: 0.6225 - 21s/epoch - 251ms/step\n",
      "Epoch 26/80\n",
      "84/84 - 21s - loss: 1.2384 - accuracy: 0.6308 - 21s/epoch - 252ms/step\n",
      "Epoch 27/80\n",
      "84/84 - 21s - loss: 1.2100 - accuracy: 0.6384 - 21s/epoch - 248ms/step\n",
      "Epoch 28/80\n",
      "84/84 - 23s - loss: 1.1806 - accuracy: 0.6470 - 23s/epoch - 271ms/step\n",
      "Epoch 29/80\n",
      "84/84 - 22s - loss: 1.1543 - accuracy: 0.6546 - 22s/epoch - 260ms/step\n",
      "Epoch 30/80\n",
      "84/84 - 21s - loss: 1.1255 - accuracy: 0.6635 - 21s/epoch - 249ms/step\n",
      "Epoch 31/80\n",
      "84/84 - 21s - loss: 1.0953 - accuracy: 0.6724 - 21s/epoch - 251ms/step\n",
      "Epoch 32/80\n",
      "84/84 - 21s - loss: 1.0662 - accuracy: 0.6817 - 21s/epoch - 250ms/step\n",
      "Epoch 33/80\n",
      "84/84 - 21s - loss: 1.0406 - accuracy: 0.6889 - 21s/epoch - 247ms/step\n",
      "Epoch 34/80\n",
      "84/84 - 22s - loss: 1.0129 - accuracy: 0.6973 - 22s/epoch - 256ms/step\n",
      "Epoch 35/80\n",
      "84/84 - 21s - loss: 0.9812 - accuracy: 0.7077 - 21s/epoch - 252ms/step\n",
      "Epoch 36/80\n",
      "84/84 - 22s - loss: 0.9550 - accuracy: 0.7154 - 22s/epoch - 257ms/step\n",
      "Epoch 37/80\n",
      "84/84 - 22s - loss: 0.9255 - accuracy: 0.7241 - 22s/epoch - 256ms/step\n",
      "Epoch 38/80\n",
      "84/84 - 21s - loss: 0.8997 - accuracy: 0.7325 - 21s/epoch - 254ms/step\n",
      "Epoch 39/80\n",
      "84/84 - 21s - loss: 0.8754 - accuracy: 0.7393 - 21s/epoch - 254ms/step\n",
      "Epoch 40/80\n",
      "84/84 - 22s - loss: 0.8435 - accuracy: 0.7498 - 22s/epoch - 256ms/step\n",
      "Epoch 41/80\n",
      "84/84 - 22s - loss: 0.8159 - accuracy: 0.7579 - 22s/epoch - 257ms/step\n",
      "Epoch 42/80\n",
      "84/84 - 22s - loss: 0.7929 - accuracy: 0.7652 - 22s/epoch - 259ms/step\n",
      "Epoch 43/80\n",
      "84/84 - 22s - loss: 0.7646 - accuracy: 0.7743 - 22s/epoch - 258ms/step\n",
      "Epoch 44/80\n",
      "84/84 - 22s - loss: 0.7349 - accuracy: 0.7841 - 22s/epoch - 262ms/step\n",
      "Epoch 45/80\n",
      "84/84 - 21s - loss: 0.7128 - accuracy: 0.7911 - 21s/epoch - 255ms/step\n",
      "Epoch 46/80\n",
      "84/84 - 22s - loss: 0.6837 - accuracy: 0.8015 - 22s/epoch - 257ms/step\n",
      "Epoch 47/80\n",
      "84/84 - 22s - loss: 0.6694 - accuracy: 0.8054 - 22s/epoch - 257ms/step\n",
      "Epoch 48/80\n",
      "84/84 - 25s - loss: 0.6328 - accuracy: 0.8178 - 25s/epoch - 300ms/step\n",
      "Epoch 49/80\n",
      "84/84 - 23s - loss: 0.6109 - accuracy: 0.8255 - 23s/epoch - 275ms/step\n",
      "Epoch 50/80\n",
      "84/84 - 23s - loss: 0.5947 - accuracy: 0.8290 - 23s/epoch - 276ms/step\n",
      "Epoch 51/80\n",
      "84/84 - 23s - loss: 0.5676 - accuracy: 0.8383 - 23s/epoch - 269ms/step\n",
      "Epoch 52/80\n",
      "84/84 - 22s - loss: 0.5517 - accuracy: 0.8431 - 22s/epoch - 263ms/step\n",
      "Epoch 53/80\n",
      "84/84 - 23s - loss: 0.5333 - accuracy: 0.8490 - 23s/epoch - 276ms/step\n",
      "Epoch 54/80\n",
      "84/84 - 23s - loss: 0.5043 - accuracy: 0.8600 - 23s/epoch - 270ms/step\n",
      "Epoch 55/80\n",
      "84/84 - 26s - loss: 0.4949 - accuracy: 0.8609 - 26s/epoch - 315ms/step\n",
      "Epoch 56/80\n",
      "84/84 - 22s - loss: 0.4704 - accuracy: 0.8697 - 22s/epoch - 259ms/step\n",
      "Epoch 57/80\n",
      "84/84 - 22s - loss: 0.4480 - accuracy: 0.8780 - 22s/epoch - 259ms/step\n",
      "Epoch 58/80\n",
      "84/84 - 22s - loss: 0.4244 - accuracy: 0.8863 - 22s/epoch - 264ms/step\n",
      "Epoch 59/80\n",
      "84/84 - 22s - loss: 0.4183 - accuracy: 0.8870 - 22s/epoch - 260ms/step\n",
      "Epoch 60/80\n",
      "84/84 - 21s - loss: 0.3997 - accuracy: 0.8923 - 21s/epoch - 250ms/step\n",
      "Epoch 61/80\n",
      "84/84 - 22s - loss: 0.3763 - accuracy: 0.9003 - 22s/epoch - 257ms/step\n",
      "Epoch 62/80\n",
      "84/84 - 21s - loss: 0.3689 - accuracy: 0.9027 - 21s/epoch - 253ms/step\n",
      "Epoch 63/80\n",
      "84/84 - 21s - loss: 0.3425 - accuracy: 0.9119 - 21s/epoch - 254ms/step\n",
      "Epoch 64/80\n",
      "84/84 - 21s - loss: 0.3296 - accuracy: 0.9158 - 21s/epoch - 254ms/step\n",
      "Epoch 65/80\n",
      "84/84 - 22s - loss: 0.3210 - accuracy: 0.9180 - 22s/epoch - 259ms/step\n",
      "Epoch 66/80\n",
      "84/84 - 21s - loss: 0.3160 - accuracy: 0.9187 - 21s/epoch - 252ms/step\n",
      "Epoch 67/80\n",
      "84/84 - 22s - loss: 0.2934 - accuracy: 0.9264 - 22s/epoch - 259ms/step\n",
      "Epoch 68/80\n",
      "84/84 - 22s - loss: 0.2773 - accuracy: 0.9319 - 22s/epoch - 259ms/step\n",
      "Epoch 69/80\n",
      "84/84 - 22s - loss: 0.2582 - accuracy: 0.9380 - 22s/epoch - 256ms/step\n",
      "Epoch 70/80\n",
      "84/84 - 21s - loss: 0.2497 - accuracy: 0.9408 - 21s/epoch - 252ms/step\n",
      "Epoch 71/80\n",
      "84/84 - 21s - loss: 0.2450 - accuracy: 0.9407 - 21s/epoch - 253ms/step\n",
      "Epoch 72/80\n",
      "84/84 - 21s - loss: 0.2418 - accuracy: 0.9412 - 21s/epoch - 252ms/step\n",
      "Epoch 73/80\n",
      "84/84 - 21s - loss: 0.2934 - accuracy: 0.9189 - 21s/epoch - 255ms/step\n",
      "Epoch 74/80\n",
      "84/84 - 22s - loss: 0.2439 - accuracy: 0.9382 - 22s/epoch - 256ms/step\n",
      "Epoch 75/80\n",
      "84/84 - 21s - loss: 0.2247 - accuracy: 0.9452 - 21s/epoch - 248ms/step\n",
      "Epoch 76/80\n",
      "84/84 - 21s - loss: 0.2127 - accuracy: 0.9487 - 21s/epoch - 253ms/step\n",
      "Epoch 77/80\n",
      "84/84 - 21s - loss: 0.1964 - accuracy: 0.9537 - 21s/epoch - 247ms/step\n",
      "Epoch 78/80\n",
      "84/84 - 21s - loss: 0.1883 - accuracy: 0.9555 - 21s/epoch - 247ms/step\n",
      "Epoch 79/80\n",
      "84/84 - 21s - loss: 0.1769 - accuracy: 0.9583 - 21s/epoch - 246ms/step\n",
      "Epoch 80/80\n",
      "84/84 - 21s - loss: 0.1726 - accuracy: 0.9591 - 21s/epoch - 249ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24f0326e0d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed\n",
    "\n",
    "hidden_units = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_units, input_shape=(None, train_X.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(hidden_units, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, epochs=80, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2594e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 문자를 주면 다음 문자를 계속해서 생성해내는 sentence_generation 함수 생성\n",
    "def sentence_generation(model, length): #학습한 모델, 다음 문자를 몇 번 생성할 것인지\n",
    "    # 문자에 대한 랜덤한 정수 생성\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "\n",
    "    # 랜덤한 정수로부터 맵핑되는 문자 생성\n",
    "    y_char = [index_to_char[ix[-1]]]\n",
    "    print(ix[-1],'번 문자',y_char[-1],'로 예측을 시작!')\n",
    "\n",
    "    # (1, length, 55) 크기의 X 생성. 즉, LSTM의 입력 시퀀스 생성\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "\n",
    "    for i in range(length):\n",
    "        # X[0][i][예측한 문자의 인덱스] = 1, 즉, 예측 문자를 다음 입력 시퀀스에 추가\n",
    "        X[0][i][ix[-1]] = 1\n",
    "        print(index_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(index_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "815baa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 번 문자 . 로 예측을 시작!\n",
      "1/1 [==============================] - 1s 682ms/step\n",
      "1/1 [==============================] - 1s 639ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      ". the rabbit shidd mide a such an fee of them gonsone the best wite speaked it wantther mouse, purzin\n"
     ]
    }
   ],
   "source": [
    "result = sentence_generation(model, 100)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad419c",
   "metadata": {},
   "source": [
    "### 문자 단위 RNN(Char RNN)으로 텍스트 생성하기\n",
    "- 다 대 일 구조의 RNN을 문자 단위로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4161ceb",
   "metadata": {},
   "source": [
    "#### 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af0c5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74cf8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의로 만든 엉터리 노래 가사\n",
    "# 프로그래머이자 맥주라는 단어를 좋아하지만 와인을 갈망한다\n",
    "raw_text = '''\n",
    "I get on with life as a programmer,\n",
    "I like to contemplate beer.\n",
    "But when I start to daydream,\n",
    "My mind turns straight to wine.\n",
    "\n",
    "Do I love wine more than beer?\n",
    "\n",
    "I like to use words about beer.\n",
    "But when I stop my talking,\n",
    "My mind turns straight to wine.\n",
    "\n",
    "I hate bugs and errors.\n",
    "But I just think back to wine,\n",
    "And I'm happy once again.\n",
    "\n",
    "I like to hang out with programming and deep learning.\n",
    "But when left alone,\n",
    "My mind turns straight to wine.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d8425a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I get on with life as a programmer, I like to contemplate beer. But when I start to daydream, My mind turns straight to wine. Do I love wine more than beer? I like to use words about beer. But when I stop my talking, My mind turns straight to wine. I hate bugs and errors. But I just think back to wine, And I'm happy once again. I like to hang out with programming and deep learning. But when left alone, My mind turns straight to wine.\n"
     ]
    }
   ],
   "source": [
    "# 단락 구분을 없애고 하나의 문자열로 재저장\n",
    "tokens = raw_text.split()\n",
    "raw_text = ' '.join(tokens)\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70fe3808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합 : [' ', \"'\", ',', '.', '?', 'A', 'B', 'D', 'I', 'M', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
      "문자 집합의 크기 : 33\n"
     ]
    }
   ],
   "source": [
    "# 중복을 제거한 문자 집합 생성 (모든 알파벳이 들어있지 않고, 대소문자 구분함)\n",
    "char_vocab = sorted(list(set(raw_text)))\n",
    "vocab_size = len(char_vocab)\n",
    "print('문자 집합 :',char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a23799d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, \"'\": 1, ',': 2, '.': 3, '?': 4, 'A': 5, 'B': 6, 'D': 7, 'I': 8, 'M': 9, 'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15, 'g': 16, 'h': 17, 'i': 18, 'j': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'o': 24, 'p': 25, 'r': 26, 's': 27, 't': 28, 'u': 29, 'v': 30, 'w': 31, 'y': 32}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = dict((char, index) for index, char in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a49fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 훈련 샘플의 수: 426\n"
     ]
    }
   ],
   "source": [
    "# 입력 시퀀스의 길이가 10가 되도록 데이터를 구성\n",
    "length = 11 #예측 대상 문자 포함\n",
    "sequences = []\n",
    "for i in range(length, len(raw_text)):\n",
    "    seq = raw_text[i-length:i] # 길이 11의 문자열을 지속적으로 만든다.\n",
    "    sequences.append(seq)\n",
    "print('총 훈련 샘플의 수: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10c94dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I get on wi',\n",
       " ' get on wit',\n",
       " 'get on with',\n",
       " 'et on with ',\n",
       " 't on with l',\n",
       " ' on with li',\n",
       " 'on with lif',\n",
       " 'n with life',\n",
       " ' with life ',\n",
       " 'with life a']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10개만 출력\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76f658bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩 수행\n",
    "encoded_sequences = []\n",
    "for sequence in sequences: # 전체 데이터에서 문장 샘플을 1개씩 꺼낸다.\n",
    "    encoded_sequence = [char_to_index[char] for char in sequence] # 문장 샘플에서 각 문자에 대해서 정수 인코딩을 수행.\n",
    "    encoded_sequences.append(encoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88bdc1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 0, 16, 14, 28, 0, 24, 23, 0, 31, 18],\n",
       " [0, 16, 14, 28, 0, 24, 23, 0, 31, 18, 28],\n",
       " [16, 14, 28, 0, 24, 23, 0, 31, 18, 28, 17],\n",
       " [14, 28, 0, 24, 23, 0, 31, 18, 28, 17, 0],\n",
       " [28, 0, 24, 23, 0, 31, 18, 28, 17, 0, 21]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "689e3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 문자 분리\n",
    "encoded_sequences = np.array(encoded_sequences)\n",
    "\n",
    "# 맨 마지막 위치의 문자를 분리\n",
    "X_data = encoded_sequences[:,:-1]\n",
    "# 맨 마지막 위치의 문자를 저장\n",
    "y_data = encoded_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "959b51a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  0 16 14 28  0 24 23  0 31]\n",
      " [ 0 16 14 28  0 24 23  0 31 18]\n",
      " [16 14 28  0 24 23  0 31 18 28]\n",
      " [14 28  0 24 23  0 31 18 28 17]\n",
      " [28  0 24 23  0 31 18 28 17  0]]\n",
      "[18 28 17  0 21]\n"
     ]
    }
   ],
   "source": [
    "print(X_data[:5])\n",
    "print(y_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bca89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X와 y 원-핫 인코딩 수행\n",
    "X_data_one_hot = [to_categorical(encoded, num_classes=vocab_size) for encoded in X_data]\n",
    "X_data_one_hot = np.array(X_data_one_hot)\n",
    "y_data_one_hot = to_categorical(y_data, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abf392b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 10, 33)\n"
     ]
    }
   ],
   "source": [
    "print(X_data_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b4285",
   "metadata": {},
   "source": [
    "- 샘플 수: 426개\n",
    "- 입력 시퀀스 길이: 10\n",
    "- 벡터의 차원: 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c24ea2",
   "metadata": {},
   "source": [
    "#### 모델 설계하기\n",
    "- 하이퍼파라미터인 은닉 상태의 크기는 64\n",
    "- 다 대 일 구조의 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcfe7913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 - 2s - loss: 3.4754 - accuracy: 0.0986 - 2s/epoch - 132ms/step\n",
      "Epoch 2/100\n",
      "14/14 - 0s - loss: 3.3635 - accuracy: 0.1972 - 43ms/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "14/14 - 0s - loss: 3.1319 - accuracy: 0.1972 - 53ms/epoch - 4ms/step\n",
      "Epoch 4/100\n",
      "14/14 - 0s - loss: 3.0115 - accuracy: 0.1972 - 48ms/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "14/14 - 0s - loss: 2.9651 - accuracy: 0.1972 - 45ms/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "14/14 - 0s - loss: 2.9459 - accuracy: 0.1972 - 45ms/epoch - 3ms/step\n",
      "Epoch 7/100\n",
      "14/14 - 0s - loss: 2.9274 - accuracy: 0.1972 - 44ms/epoch - 3ms/step\n",
      "Epoch 8/100\n",
      "14/14 - 0s - loss: 2.9086 - accuracy: 0.1972 - 45ms/epoch - 3ms/step\n",
      "Epoch 9/100\n",
      "14/14 - 0s - loss: 2.8892 - accuracy: 0.1972 - 46ms/epoch - 3ms/step\n",
      "Epoch 10/100\n",
      "14/14 - 0s - loss: 2.8596 - accuracy: 0.1972 - 44ms/epoch - 3ms/step\n",
      "Epoch 11/100\n",
      "14/14 - 0s - loss: 2.8269 - accuracy: 0.1972 - 45ms/epoch - 3ms/step\n",
      "Epoch 12/100\n",
      "14/14 - 0s - loss: 2.7843 - accuracy: 0.2019 - 45ms/epoch - 3ms/step\n",
      "Epoch 13/100\n",
      "14/14 - 0s - loss: 2.7457 - accuracy: 0.2136 - 45ms/epoch - 3ms/step\n",
      "Epoch 14/100\n",
      "14/14 - 0s - loss: 2.7046 - accuracy: 0.2066 - 44ms/epoch - 3ms/step\n",
      "Epoch 15/100\n",
      "14/14 - 0s - loss: 2.6701 - accuracy: 0.2324 - 44ms/epoch - 3ms/step\n",
      "Epoch 16/100\n",
      "14/14 - 0s - loss: 2.6158 - accuracy: 0.2394 - 45ms/epoch - 3ms/step\n",
      "Epoch 17/100\n",
      "14/14 - 0s - loss: 2.5892 - accuracy: 0.2465 - 44ms/epoch - 3ms/step\n",
      "Epoch 18/100\n",
      "14/14 - 0s - loss: 2.5306 - accuracy: 0.2723 - 46ms/epoch - 3ms/step\n",
      "Epoch 19/100\n",
      "14/14 - 0s - loss: 2.4869 - accuracy: 0.2911 - 49ms/epoch - 3ms/step\n",
      "Epoch 20/100\n",
      "14/14 - 0s - loss: 2.4616 - accuracy: 0.2653 - 46ms/epoch - 3ms/step\n",
      "Epoch 21/100\n",
      "14/14 - 0s - loss: 2.4165 - accuracy: 0.3005 - 44ms/epoch - 3ms/step\n",
      "Epoch 22/100\n",
      "14/14 - 0s - loss: 2.3882 - accuracy: 0.3099 - 46ms/epoch - 3ms/step\n",
      "Epoch 23/100\n",
      "14/14 - 0s - loss: 2.3353 - accuracy: 0.3169 - 46ms/epoch - 3ms/step\n",
      "Epoch 24/100\n",
      "14/14 - 0s - loss: 2.3055 - accuracy: 0.3357 - 45ms/epoch - 3ms/step\n",
      "Epoch 25/100\n",
      "14/14 - 0s - loss: 2.2847 - accuracy: 0.3028 - 47ms/epoch - 3ms/step\n",
      "Epoch 26/100\n",
      "14/14 - 0s - loss: 2.2245 - accuracy: 0.3427 - 47ms/epoch - 3ms/step\n",
      "Epoch 27/100\n",
      "14/14 - 0s - loss: 2.1849 - accuracy: 0.3592 - 47ms/epoch - 3ms/step\n",
      "Epoch 28/100\n",
      "14/14 - 0s - loss: 2.1615 - accuracy: 0.3568 - 49ms/epoch - 4ms/step\n",
      "Epoch 29/100\n",
      "14/14 - 0s - loss: 2.1160 - accuracy: 0.3803 - 49ms/epoch - 3ms/step\n",
      "Epoch 30/100\n",
      "14/14 - 0s - loss: 2.0918 - accuracy: 0.3850 - 49ms/epoch - 4ms/step\n",
      "Epoch 31/100\n",
      "14/14 - 0s - loss: 2.0540 - accuracy: 0.4343 - 49ms/epoch - 3ms/step\n",
      "Epoch 32/100\n",
      "14/14 - 0s - loss: 2.0127 - accuracy: 0.4202 - 48ms/epoch - 3ms/step\n",
      "Epoch 33/100\n",
      "14/14 - 0s - loss: 1.9792 - accuracy: 0.4577 - 44ms/epoch - 3ms/step\n",
      "Epoch 34/100\n",
      "14/14 - 0s - loss: 1.9436 - accuracy: 0.4577 - 45ms/epoch - 3ms/step\n",
      "Epoch 35/100\n",
      "14/14 - 0s - loss: 1.9089 - accuracy: 0.4648 - 46ms/epoch - 3ms/step\n",
      "Epoch 36/100\n",
      "14/14 - 0s - loss: 1.8596 - accuracy: 0.4977 - 44ms/epoch - 3ms/step\n",
      "Epoch 37/100\n",
      "14/14 - 0s - loss: 1.8129 - accuracy: 0.5305 - 45ms/epoch - 3ms/step\n",
      "Epoch 38/100\n",
      "14/14 - 0s - loss: 1.7980 - accuracy: 0.5188 - 44ms/epoch - 3ms/step\n",
      "Epoch 39/100\n",
      "14/14 - 0s - loss: 1.7694 - accuracy: 0.5000 - 47ms/epoch - 3ms/step\n",
      "Epoch 40/100\n",
      "14/14 - 0s - loss: 1.7186 - accuracy: 0.5282 - 47ms/epoch - 3ms/step\n",
      "Epoch 41/100\n",
      "14/14 - 0s - loss: 1.6909 - accuracy: 0.5376 - 52ms/epoch - 4ms/step\n",
      "Epoch 42/100\n",
      "14/14 - 0s - loss: 1.6450 - accuracy: 0.5610 - 52ms/epoch - 4ms/step\n",
      "Epoch 43/100\n",
      "14/14 - 0s - loss: 1.6340 - accuracy: 0.5563 - 47ms/epoch - 3ms/step\n",
      "Epoch 44/100\n",
      "14/14 - 0s - loss: 1.6167 - accuracy: 0.5798 - 46ms/epoch - 3ms/step\n",
      "Epoch 45/100\n",
      "14/14 - 0s - loss: 1.5553 - accuracy: 0.5915 - 45ms/epoch - 3ms/step\n",
      "Epoch 46/100\n",
      "14/14 - 0s - loss: 1.5181 - accuracy: 0.5986 - 46ms/epoch - 3ms/step\n",
      "Epoch 47/100\n",
      "14/14 - 0s - loss: 1.5000 - accuracy: 0.5986 - 68ms/epoch - 5ms/step\n",
      "Epoch 48/100\n",
      "14/14 - 0s - loss: 1.4436 - accuracy: 0.6479 - 44ms/epoch - 3ms/step\n",
      "Epoch 49/100\n",
      "14/14 - 0s - loss: 1.4149 - accuracy: 0.6385 - 44ms/epoch - 3ms/step\n",
      "Epoch 50/100\n",
      "14/14 - 0s - loss: 1.4166 - accuracy: 0.6479 - 43ms/epoch - 3ms/step\n",
      "Epoch 51/100\n",
      "14/14 - 0s - loss: 1.3908 - accuracy: 0.6667 - 43ms/epoch - 3ms/step\n",
      "Epoch 52/100\n",
      "14/14 - 0s - loss: 1.3364 - accuracy: 0.6808 - 44ms/epoch - 3ms/step\n",
      "Epoch 53/100\n",
      "14/14 - 0s - loss: 1.2834 - accuracy: 0.6737 - 45ms/epoch - 3ms/step\n",
      "Epoch 54/100\n",
      "14/14 - 0s - loss: 1.2604 - accuracy: 0.7042 - 43ms/epoch - 3ms/step\n",
      "Epoch 55/100\n",
      "14/14 - 0s - loss: 1.2150 - accuracy: 0.7019 - 43ms/epoch - 3ms/step\n",
      "Epoch 56/100\n",
      "14/14 - 0s - loss: 1.1884 - accuracy: 0.7230 - 44ms/epoch - 3ms/step\n",
      "Epoch 57/100\n",
      "14/14 - 0s - loss: 1.1487 - accuracy: 0.7465 - 44ms/epoch - 3ms/step\n",
      "Epoch 58/100\n",
      "14/14 - 0s - loss: 1.1284 - accuracy: 0.7324 - 46ms/epoch - 3ms/step\n",
      "Epoch 59/100\n",
      "14/14 - 0s - loss: 1.0981 - accuracy: 0.7582 - 46ms/epoch - 3ms/step\n",
      "Epoch 60/100\n",
      "14/14 - 0s - loss: 1.0570 - accuracy: 0.7700 - 46ms/epoch - 3ms/step\n",
      "Epoch 61/100\n",
      "14/14 - 0s - loss: 1.0442 - accuracy: 0.7700 - 46ms/epoch - 3ms/step\n",
      "Epoch 62/100\n",
      "14/14 - 0s - loss: 1.0053 - accuracy: 0.7887 - 47ms/epoch - 3ms/step\n",
      "Epoch 63/100\n",
      "14/14 - 0s - loss: 0.9929 - accuracy: 0.7864 - 45ms/epoch - 3ms/step\n",
      "Epoch 64/100\n",
      "14/14 - 0s - loss: 0.9560 - accuracy: 0.7934 - 46ms/epoch - 3ms/step\n",
      "Epoch 65/100\n",
      "14/14 - 0s - loss: 0.9375 - accuracy: 0.7958 - 46ms/epoch - 3ms/step\n",
      "Epoch 66/100\n",
      "14/14 - 0s - loss: 0.9098 - accuracy: 0.8192 - 45ms/epoch - 3ms/step\n",
      "Epoch 67/100\n",
      "14/14 - 0s - loss: 0.8650 - accuracy: 0.8239 - 47ms/epoch - 3ms/step\n",
      "Epoch 68/100\n",
      "14/14 - 0s - loss: 0.8377 - accuracy: 0.8239 - 46ms/epoch - 3ms/step\n",
      "Epoch 69/100\n",
      "14/14 - 0s - loss: 0.8017 - accuracy: 0.8474 - 44ms/epoch - 3ms/step\n",
      "Epoch 70/100\n",
      "14/14 - 0s - loss: 0.7786 - accuracy: 0.8732 - 43ms/epoch - 3ms/step\n",
      "Epoch 71/100\n",
      "14/14 - 0s - loss: 0.7518 - accuracy: 0.8685 - 47ms/epoch - 3ms/step\n",
      "Epoch 72/100\n",
      "14/14 - 0s - loss: 0.7248 - accuracy: 0.8615 - 44ms/epoch - 3ms/step\n",
      "Epoch 73/100\n",
      "14/14 - 0s - loss: 0.7128 - accuracy: 0.8756 - 44ms/epoch - 3ms/step\n",
      "Epoch 74/100\n",
      "14/14 - 0s - loss: 0.7048 - accuracy: 0.8779 - 43ms/epoch - 3ms/step\n",
      "Epoch 75/100\n",
      "14/14 - 0s - loss: 0.6779 - accuracy: 0.8897 - 43ms/epoch - 3ms/step\n",
      "Epoch 76/100\n",
      "14/14 - 0s - loss: 0.6503 - accuracy: 0.9131 - 45ms/epoch - 3ms/step\n",
      "Epoch 77/100\n",
      "14/14 - 0s - loss: 0.6244 - accuracy: 0.9202 - 47ms/epoch - 3ms/step\n",
      "Epoch 78/100\n",
      "14/14 - 0s - loss: 0.6067 - accuracy: 0.9085 - 47ms/epoch - 3ms/step\n",
      "Epoch 79/100\n",
      "14/14 - 0s - loss: 0.6022 - accuracy: 0.9202 - 46ms/epoch - 3ms/step\n",
      "Epoch 80/100\n",
      "14/14 - 0s - loss: 0.5719 - accuracy: 0.9296 - 44ms/epoch - 3ms/step\n",
      "Epoch 81/100\n",
      "14/14 - 0s - loss: 0.5480 - accuracy: 0.9484 - 47ms/epoch - 3ms/step\n",
      "Epoch 82/100\n",
      "14/14 - 0s - loss: 0.5409 - accuracy: 0.9319 - 46ms/epoch - 3ms/step\n",
      "Epoch 83/100\n",
      "14/14 - 0s - loss: 0.5103 - accuracy: 0.9413 - 47ms/epoch - 3ms/step\n",
      "Epoch 84/100\n",
      "14/14 - 0s - loss: 0.4982 - accuracy: 0.9343 - 43ms/epoch - 3ms/step\n",
      "Epoch 85/100\n",
      "14/14 - 0s - loss: 0.4951 - accuracy: 0.9296 - 42ms/epoch - 3ms/step\n",
      "Epoch 86/100\n",
      "14/14 - 0s - loss: 0.4742 - accuracy: 0.9531 - 45ms/epoch - 3ms/step\n",
      "Epoch 87/100\n",
      "14/14 - 0s - loss: 0.4538 - accuracy: 0.9507 - 45ms/epoch - 3ms/step\n",
      "Epoch 88/100\n",
      "14/14 - 0s - loss: 0.4415 - accuracy: 0.9601 - 45ms/epoch - 3ms/step\n",
      "Epoch 89/100\n",
      "14/14 - 0s - loss: 0.4208 - accuracy: 0.9671 - 47ms/epoch - 3ms/step\n",
      "Epoch 90/100\n",
      "14/14 - 0s - loss: 0.3998 - accuracy: 0.9601 - 47ms/epoch - 3ms/step\n",
      "Epoch 91/100\n",
      "14/14 - 0s - loss: 0.3944 - accuracy: 0.9648 - 49ms/epoch - 3ms/step\n",
      "Epoch 92/100\n",
      "14/14 - 0s - loss: 0.3867 - accuracy: 0.9577 - 46ms/epoch - 3ms/step\n",
      "Epoch 93/100\n",
      "14/14 - 0s - loss: 0.3669 - accuracy: 0.9742 - 47ms/epoch - 3ms/step\n",
      "Epoch 94/100\n",
      "14/14 - 0s - loss: 0.3549 - accuracy: 0.9695 - 44ms/epoch - 3ms/step\n",
      "Epoch 95/100\n",
      "14/14 - 0s - loss: 0.3490 - accuracy: 0.9718 - 45ms/epoch - 3ms/step\n",
      "Epoch 96/100\n",
      "14/14 - 0s - loss: 0.3391 - accuracy: 0.9718 - 51ms/epoch - 4ms/step\n",
      "Epoch 97/100\n",
      "14/14 - 0s - loss: 0.3211 - accuracy: 0.9765 - 51ms/epoch - 4ms/step\n",
      "Epoch 98/100\n",
      "14/14 - 0s - loss: 0.3099 - accuracy: 0.9742 - 49ms/epoch - 3ms/step\n",
      "Epoch 99/100\n",
      "14/14 - 0s - loss: 0.2987 - accuracy: 0.9695 - 49ms/epoch - 4ms/step\n",
      "Epoch 100/100\n",
      "14/14 - 0s - loss: 0.2903 - accuracy: 0.9765 - 46ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24f126aa070>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "hidden_units = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_units, input_shape=(X_data_one_hot.shape[1], X_data_one_hot.shape[2])))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_data_one_hot, y_data_one_hot, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71aa6104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 생성하는 함수 sentence_generation을 만들어서 문장을 생성\n",
    "def sentence_generation(model, char_to_index, seq_length, seed_text, n):\n",
    "\n",
    "    # 초기 시퀀스\n",
    "    init_text = seed_text\n",
    "    sentence = ''\n",
    "\n",
    "    # 다음 문자 예측은 총 n번만 반복.\n",
    "    for _ in range(n):\n",
    "        encoded = [char_to_index[char] for char in seed_text] # 현재 시퀀스에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, padding='pre') # 데이터에 대한 패딩\n",
    "        encoded = to_categorical(encoded, num_classes=len(char_to_index))\n",
    "\n",
    "        # 입력한 X(현재 시퀀스)에 대해서 y를 예측하고 y(예측한 문자)를 result에 저장.\n",
    "        result = model.predict(encoded, verbose=0)\n",
    "        result = np.argmax(result, axis=1)\n",
    "\n",
    "        for char, index in char_to_index.items():\n",
    "            if index == result:\n",
    "                break\n",
    "\n",
    "        # 현재 시퀀스 + 예측 문자를 현재 시퀀스로 변경\n",
    "        seed_text = seed_text + char\n",
    "\n",
    "        # 예측 문자를 문장에 저장\n",
    "        sentence = sentence + char\n",
    "\n",
    "    # n번의 다음 문자 예측이 끝나면 최종 완성된 문장을 리턴.\n",
    "    sentence = init_text + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d4b6e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I get on with life as a programmer, I like to use words about beer. But when I start to da\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, char_to_index, 10, 'I get on w', 80))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "378.458px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
